# Data Analytics LifeCycle

---

## Phase 1: Discovery: Problem Formulation

### 1.1 Business Domain Understanding

#### 1.1.1 StakeHolder interviews:
- consult with stakeholders to understand the business problem
- determine the business goals and objectives
- ask them their pain points
- Key Question: Is this a descriptive problem (What happened?) or predictive (What will happen?).

#### 1.1.2 Understanding Current State:
- audit existing resources: do we have hardware(gpu, server, etc)?
- audit skill sets: do we need a data engineer or a data scientist?
- in short, analyze what we have and what we need.

### 1.2 Problem Statement Definition:

#### 1.2.1 SMART goal framework:
- Specific: Reduce customer churns. (What are we trying to measure?)
- Measurable: By 5%. (How will we know if we have succeeded?)
- Achievable: Based on historical trends. (Is this a realistic goal?)
- Relevant: Impact revenue directly. (Is this goal in line with the business goals?)
- Time-bound: within Q3 2025 (Is this goal within our timeframe?)

(upper give this information is just for understanding purpose)

#### 1.2.2 Hypothesis Formulation:
- Null Hypothesis(H0): There is no significant difference between the number of churned customers and the number of active customers.
- Alternative Hypothesis(H1): There is a significant difference between the number of churned customers and the number of active customers.

---

## Phase 2: Data Preparation (Data Munging) :

###### this is the most time-consuming phase it takes 60â€“80% efforts

### 2.1 Data Ingestion (Collection):

#### 2.1.1 Source Identification:
- Internal: SQL Databases (Transaction logs), CRM systems, etc.
- External: Third party API, Public Datasets, Web Scraping, etc.

#### 2.1.2 Extractions Methods:
- Batch Processing: Downloading data once a day. (in simple we extract all data once at a time like in a certain time)(e.g.: nightly csv dumps)
- Streaming Processing: Real-time data ingestion. (we're continuously extracting new data from such platforms) (e.g.: Kafka, Kinesis, etc.)

### 2.2 Data Assessment: (Profiling)

#### 2.2.1 Structural check:
- check data types. (e.g.: age is int or sting, price is float or string)
- check volume. (rows/columns count)

#### 2.2.2 Quality Check:
- identify missing values. (Nulls/NaNs)
- identify outliers.
- identify duplicates.

### 2.3 Data Cleaning: (Sanitization)

#### 2.3.1 Missing Data

###### we have two methods here for handle missing data
- Method A: Dropping: Remove rows with missing values. (but for some cases it's not a good idea cause some time we have more missing values it makes our data half or more lower)
- Method B: Imputation(filling): 
    - mean/median: for numerical data. (e.g.: fill age with average age)
    - mode: for categorical data.
    - predictive: using KNN(K nearest neighbors) to guess missing values based on similarity.

#### 2.3.2 Outliers treatment:
- detection: using box plots(IQR methods) or Z-Scores(3>standard deviation).
- action: cap the outliers(winsorization) or remove them if they are errors.

### 2.4 Data Transformation: (Feature Engineering)

#### 2.4.1 Normalization/Standardization:
- scaling data so big numbers (Salary: 50,000) doesn't dominate small numbers (Age: 25).
- techniques: Min-Max Scaling (0-1), Z-Score Standardization.

#### 2.4.2 Feature Engineering:
- Binning: Converting Age(11,23,13) to group(10-19, 20-29, 30+).
- Encoding: converting text to numbers for the machine (because the machine can't understand text, it only grasps numbers)
    - One-Hot Encoding: converting categorical data to binary vectors. (e.g.: male=1, female=0)
    - Label Encoding: converting categorical data to numerical values. (e.g.: male=1, female=2)

---

## Phase 3: Model Planning: EDA ~ Exploratory Data Analysis

###### understanding the shape of data

### 3.1 Univariate Analysis: (one variable at a time)

#### 3.1.1 Central tendency:
- mean, median, mode.

#### 3.1.2 Dispersion:
- standard deviation, variance, range.

#### 3.1.3 Distribution:
- skewness, kurtosis.

### 3.2 Bivariate Analysis: (two variables at a time)

#### 3.2.1 Correlation matrix:
- Use Pearson Correlation Coefficient for numeric data(-1 to +1).
- Goal: Detect Multicollinearity (if two inputs are too similar, drop one to avoid confusing the model).

#### 3.2.2 Visualization:
- Use scatter plot to visualize the relationship between two variables.
- Box Plots (category vs numerical)

### 3.3 Variable Selection: 

#### 3.3.1 Dimensionality Reduction:
- PCA: Principal Component Analysis: reducing the number of features to keep only the most important ones. (in short, compressing 100 features into 10 super features without losing information)
- Feature Importance: using random forest to tell you which columns matter the most.

---

## Phase 4: Model Building: (Execution)

###### running the algorithms

### 4.1 Dataset Splitting 

#### 4.1.1 The Split Ratio

- Training Set(70-80%): the data the model learns.
- Testing Set(20-30%): the exam questions the model has never seen, used to grade it.
- Validation Set: Used for tuning during training. (Optional)

### 4.2 Model Selection:

#### 4.2.1 Regression Models: (predict a number)
- Linear Regression
- Decision Tree Regressor

#### 4.2.2 Classification Models: (predict a category)
- Logistic Regression
- SVM
- Random Forest
- Neural Network

#### 4.2.3 Clustering: (grouping)
- K-Means
- DBSCAN

### 4.3 Model Execution & Tuning:

#### 4.3.1 Hyperparameter Tuning:
- adjusting the settings of the algorithm. (e.g.: Tree Depth, Learning Rate)
- Tools:
  - Grid Search: iterating through a grid of values for each parameter.
  - Random Search: iterating through a random sample of values for each parameter.


